# Simple Login Data Pipeline
Simple Login Data Pipeline consisting of 
data producer - message queue - data consumer - data monitoring


## 2nd iteration 
### Project requirements
### Main goals
### Risk assessment
- Version of Elasticsearch and Kibana: No previous experience on Kafka and producer/consumer as well
- Itegration with Spark and Elasticsearch

### Actions
### Results
### How to run
1. Clone this repo to your local host
```Shell
```

2. Create two directories for Kafka and Zookeeper at current cloned path
```Shell
```

3. Change ownership of those two directory
```Shell
```

4. Create a user-defined network 
```Shell
```

5. Run with docker-compose
```Shell
```
